# Current SDK

> **The Twilio for video+LLM** - Turn any camera stream into structured JSON + speech in 3 lines of code.

## ğŸ¯ Vision

Make "see what I see, tell me what to do next" as simple as sending a text message. We're building the infrastructure layer that makes real-time multimodal apps as easy as text chat.

## ğŸš€ MVP Goal

**Ship the most minimal SDK that proves the core value prop**: `video in â†’ JSON + speech out`

### What We're Building

No WebRTC/WebSocket/TTS/JSON plumbingâ€”batteries-included.

## âœ¨ What you get (MVP)

- **SDK (Web)**: `start()`, `stop()`, `on("instruction")`, `on("error")`, `on("state")`
- **Structured JSON** back from the model (schema-validated)
- **Optional TTS** (speak instructions)
- **Basic observability** (console logs: fps, latency, errors)
- **Demo app**: "AI Cooking Assistant" (camera on â†’ JSON cue â†’ spoken guidance)

## ğŸš€ Quickstart (dev UX)

```javascript
import { Current } from "@current/sdk";

// 1) Start session
const session = await Current.start({
  provider: "gemini",   // or "openai"
  mode: "cooking",      // prompt preset
  fps: 1,
  tts: true,            // optional speech out
});

// 2) React to instructions
session.on("instruction", (msg) => {
  // Machine-usable JSON
  console.log(msg.json);  // e.g., { cue: "stir_now", confidence: 0.87 }
  // Human-friendly text
  ui.show(msg.text);      // "Stir your eggs now"
});

// 3) Stop when done
session.stop();
```

**That's it.** Current handles camera permission, frame sampling, background send, JSON parsing, and (optionally) TTS.

## ğŸ“¡ Event Catalog (MVP)

- **`instruction`** â†’ `{ json, text, timestamp }`
- **`state`** â†’ `"connecting" | "running" | "speaking" | "stopped"`
- **`error`** â†’ `{ code, message }`
- **`metric`** (optional dev flag) â†’ `{ fps, latencyMs }` (console only for MVP)

## ğŸ“‹ JSON Schema (MVP "cooking" preset)

```json
{
  "type": "object",
  "required": ["cue", "confidence"],
  "properties": {
    "cue": {
      "type": "string",
      "enum": ["stir_now", "flip_pancake", "wait", "reduce_heat", "add_ingredient"]
    },
    "confidence": { "type": "number", "minimum": 0, "maximum": 1 },
    "ingredient": { "type": "string" },
    "note": { "type": "string" }
  },
  "additionalProperties": false
}
```

## ğŸ—ï¸ Minimal Architecture (MVP)

```
[App UI]  â† events  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                 â”‚           Current SDK (Web)              â”‚
   â”‚ start/stop      â”‚ - Camera permission + preview            â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ - Frame sampler (1 FPS)                  â”‚
   â”‚                 â”‚ - Background worker: send/receive        â”‚
   â”‚  instruction â—€â”€ â”‚ - JSON schema validate + text render     â”‚
   â”‚  state/error â—€â”€ â”‚ - Optional TTS playback                  â”‚
   â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚                                 â”‚ WS/WebRTC (one stream)
   â–¼                                 â–¼
                         Current Gateway (API) â€” thin proxy
                         - Auth / session broker
                         - Routes to Gemini Live / OpenAI Realtime
                         - Returns model messages
```

**MVP observability** = console logs from SDK. (Gateway stores nothing yet.)

## ğŸ“ File Layout (MVP)

```
current/
â”œâ”€ packages/
â”‚  â”œâ”€ sdk/                  # Web SDK
â”‚  â”‚  â”œâ”€ src/
â”‚  â”‚  â”‚  â”œâ”€ index.ts        # Current.start / events
â”‚  â”‚  â”‚  â”œâ”€ camera.ts       # getUserMedia + preview
â”‚  â”‚  â”‚  â”œâ”€ sampler.ts      # 1 FPS
â”‚  â”‚  â”‚  â”œâ”€ transport.ts    # WS client
â”‚  â”‚  â”‚  â”œâ”€ schema.ts       # AJV validator
â”‚  â”‚  â”‚  â””â”€ tts.ts          # optional (simple Web Speech or provider)
â”‚  â”‚  â””â”€ package.json
â”‚  â””â”€ gateway/              # Minimal Node/Express or Fastify
â”‚     â”œâ”€ src/index.ts       # /v1/session (create), /v1/stream (WS)
â”‚     â””â”€ package.json
â””â”€ demos/
   â””â”€ cooking-web/
      â”œâ”€ index.html
      â”œâ”€ main.ts
      â””â”€ vite.config.ts
```

## ğŸ”Œ Gateway API (MVP)

- **POST** `/v1/session` â†’ `{ sessionId, wsUrl }`
- **WS** `/v1/stream?sessionId=...` (binary/text)

Client sends sampled frames (JPEG/WebP) or (later) keypoints

Gateway forwards to provider (Gemini Live / OpenAI Realtime)

Gateway streams back model messages â†’ SDK parses to instruction

**Auth for MVP**: simple bearer token env var on gateway + client. No user accounts.

## ğŸ¤– Model Prompt (MVP "cooking")

**System:**
"You are a realtime cooking assistant. You ONLY reply with strict JSON per this schema: â€¦ (paste the JSON schema). Use short cues that are actionable for the next 2â€“5 seconds. Do not add fields. If uncertain, return { "cue": "wait", "confidence": X }."

**User (streamed context):**
- "Mode: cooking"
- "User is cooking eggs on a stovetop."
- "Frame: <image>" (sent each second)

## ğŸ¬ Demo Script (Investor-friendly)

1. Show 10 lines of app code (Quickstart)
2. Click "Start": camera turns on
3. Move pan / flip pancake â†’ console shows JSON + UI shows spoken instruction
4. "Same SDK powers fitness/tutoring/safety. Camera in â†’ JSON out."

## ğŸ¤ Contributing

We're building this in public! Check out our [implementation plan](./IMPLEMENTATION.md) and join the conversation.

## ğŸ“„ License

MIT License - see [LICENSE](./LICENSE) for details.

---

**Ready to build the future of multimodal apps?** ğŸš€

[Get Started](#getting-started) â€¢ [View Examples](./examples/) â€¢ [Join Discord](https://discord.gg/multimodal-sdk)
